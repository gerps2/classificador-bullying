{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Embedding, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse bloco de código lê os arquivos de dados, realiza o pré-processamento dos dados, equilibra as classes e limita a quantidade total de itens, preparando-os para uso em modelos de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  index                                               Text  ed_label_0  \\\n",
      "0     0  `- This is not ``creative``.  Those are the di...    0.900000   \n",
      "1     1  `  :: the term ``standard model`` is itself le...    1.000000   \n",
      "2     2    True or false, the situation as of March 200...    1.000000   \n",
      "3     3   Next, maybe you could work on being less cond...    0.555556   \n",
      "4     4               This page will need disambiguation.     1.000000   \n",
      "\n",
      "   ed_label_1  oh_label  type_text type_text_label   id Annotation  \n",
      "0    0.100000       0.0        1.0      aggression  NaN        NaN  \n",
      "1    0.000000       0.0        1.0      aggression  NaN        NaN  \n",
      "2    0.000000       0.0        1.0      aggression  NaN        NaN  \n",
      "3    0.444444       0.0        1.0      aggression  NaN        NaN  \n",
      "4    0.000000       0.0        1.0      aggression  NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "pasta_dados = 'dados'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "mapeamento_arquivos = {\n",
    "    'aggression.csv': 1,\n",
    "    'attack.csv': 2,\n",
    "    'racism.csv': 3,\n",
    "    'sexism.csv': 4,\n",
    "    'toxicity.csv': 5\n",
    "}\n",
    "\n",
    "\n",
    "for arquivo in os.listdir(pasta_dados):\n",
    "    if arquivo.endswith('.csv'):\n",
    "        caminho_arquivo = os.path.join(pasta_dados, arquivo)\n",
    "        df = pd.read_csv(caminho_arquivo)\n",
    "        \n",
    "        if arquivo in mapeamento_arquivos:\n",
    "            tipo_texto = mapeamento_arquivos[arquivo]\n",
    "            df['type_text'] = tipo_texto\n",
    "        \n",
    "        # Converter apenas as colunas numéricas para inteiros\n",
    "        colunas_numericas = df.select_dtypes(include='number').columns\n",
    "        df[colunas_numericas] = df[colunas_numericas].apply(pd.to_numeric, downcast='integer', errors='coerce')\n",
    "        \n",
    "        df['type_text_label'] = arquivo.replace(\".csv\", \"\")\n",
    "        dataframes.append(df)\n",
    "\n",
    "df_final = pd.concat(dataframes)\n",
    "\n",
    "# # Balancear as classes e limitar a quantidade total de itens\n",
    "\n",
    "# # Separa as classes majoritária e minoritária\n",
    "# df_majority = df_final[df_final.oh_label==0]\n",
    "# df_minority = df_final[df_final.oh_label==1]\n",
    "\n",
    "# # Calcula a quantidade de itens para igualar as classes\n",
    "# n_samples = 1000 // 2\n",
    "\n",
    "# # Faz a subamostragem na classe majoritária\n",
    "# df_majority_downsampled = resample(df_majority, \n",
    "#                                  replace=False,    # sample without replacement\n",
    "#                                  n_samples=n_samples,     # to match minority class\n",
    "#                                  random_state=123) # reproducible results\n",
    "\n",
    "# # Faz a superamostragem na classe minoritária\n",
    "# df_minority_upsampled = resample(df_minority, \n",
    "#                                  replace=True,     # sample with replacement\n",
    "#                                  n_samples=n_samples,    # to match majority class\n",
    "#                                  random_state=123) # reproducible results\n",
    "\n",
    "# # Combina a classe minoritária com a classe majoritária\n",
    "# df_final = pd.concat([df_majority_downsampled, df_minority_upsampled])\n",
    "\n",
    "print(df_final.head())\n",
    "# print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse trecho de código seleciona as colunas desejadas do dataframe \"df_final\" e cria um novo dataframe chamado \"data_frame\" contendo apenas essas colunas. Em seguida, ele imprime as primeiras linhas do novo dataframe para visualização. Isso pode ser útil para verificar se as colunas desejadas foram selecionadas corretamente e ter uma amostra dos dados que serão utilizados nas etapas seguintes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  oh_label  type_text  \\\n",
      "0  `- This is not ``creative``.  Those are the di...       0.0        1.0   \n",
      "1  `  :: the term ``standard model`` is itself le...       0.0        1.0   \n",
      "2    True or false, the situation as of March 200...       0.0        1.0   \n",
      "3   Next, maybe you could work on being less cond...       0.0        1.0   \n",
      "4               This page will need disambiguation.        0.0        1.0   \n",
      "\n",
      "  type_text_label  \n",
      "0      aggression  \n",
      "1      aggression  \n",
      "2      aggression  \n",
      "3      aggression  \n",
      "4      aggression  \n"
     ]
    }
   ],
   "source": [
    "colunas_desejadas = ['Text', 'oh_label', 'type_text', 'type_text_label']\n",
    "data_frame = df_final[colunas_desejadas]\n",
    "print(data_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse bloco de código remove os valores nulos das colunas 'Text' e 'oh_label', em seguida verifica e remove os itens duplicados com base na coluna 'Text' no dataframe \"data_frame\". Isso é feito para garantir a consistência dos dados e evitar duplicações que possam interferir nos resultados da análise posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de itens duplicados antes da remoção: 322486\n",
      "Quantidade de itens duplicados após a remoção: 0\n",
      "DataFrame sem itens duplicados:\n",
      "                                                Text  oh_label  type_text  \\\n",
      "0  `- This is not ``creative``.  Those are the di...       0.0        1.0   \n",
      "1  `  :: the term ``standard model`` is itself le...       0.0        1.0   \n",
      "2    True or false, the situation as of March 200...       0.0        1.0   \n",
      "3   Next, maybe you could work on being less cond...       0.0        1.0   \n",
      "4               This page will need disambiguation.        0.0        1.0   \n",
      "\n",
      "  type_text_label  \n",
      "0      aggression  \n",
      "1      aggression  \n",
      "2      aggression  \n",
      "3      aggression  \n",
      "4      aggression  \n"
     ]
    }
   ],
   "source": [
    "# Remova valores nulos das colunas 'Text' e 'type_text'\n",
    "data_frame = data_frame.dropna(subset=['Text', 'oh_label'])\n",
    "\n",
    "# Verifique a quantidade de itens duplicados antes da remoção\n",
    "qtd_duplicados = data_frame.duplicated(subset=['Text']).sum()\n",
    "print(\"Quantidade de itens duplicados antes da remoção:\", qtd_duplicados)\n",
    "\n",
    "# Remova itens duplicados pela coluna 'Text'\n",
    "data_frame = data_frame.drop_duplicates(subset=['Text'])\n",
    "\n",
    "# Verifique a quantidade de itens duplicados após a remoção\n",
    "qtd_duplicados = data_frame.duplicated(subset=['Text']).sum()\n",
    "print(\"Quantidade de itens duplicados após a remoção:\", qtd_duplicados)\n",
    "\n",
    "# Exiba o dataframe resultante\n",
    "print(\"DataFrame sem itens duplicados:\")\n",
    "print(data_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse bloco de código verifica e baixa os pacotes necessários do nltk, define uma função para processar o texto removendo caracteres não-alfabéticos, convertendo para minúsculas, tokenizando, removendo stopwords e lematizando as palavras. Em seguida, aplica essa função à coluna 'Text' do dataframe \"data_frame\" para processar os textos e exibe as primeiras linhas do dataframe resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gerso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gerso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gerso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>type_text</th>\n",
       "      <th>type_text_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>creative dictionary definition term insurance ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>term standard model le npov think prefer new a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>true false situation march saudi proposal land...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>next maybe could work le condescending suggest...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>page need disambiguation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  oh_label  type_text  \\\n",
       "0  creative dictionary definition term insurance ...       0.0        1.0   \n",
       "1  term standard model le npov think prefer new a...       0.0        1.0   \n",
       "2  true false situation march saudi proposal land...       0.0        1.0   \n",
       "3  next maybe could work le condescending suggest...       0.0        1.0   \n",
       "4                           page need disambiguation       0.0        1.0   \n",
       "\n",
       "  type_text_label  \n",
       "0      aggression  \n",
       "1      aggression  \n",
       "2      aggression  \n",
       "3      aggression  \n",
       "4      aggression  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifica se os pacotes do nltk já foram baixados\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))  # Coloca as stopwords num conjunto para buscar mais rápido\n",
    "\n",
    "def process_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)  # remove caracteres não-alfabéticos\n",
    "    text = text.lower()  # transforma o texto para minúsculo\n",
    "    words = nltk.word_tokenize(text)  # tokeniza o texto em palavras\n",
    "    words = [word for word in words if word not in stop_words]  # remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # lematiza as palavras\n",
    "    return ' '.join(words)\n",
    "\n",
    "data_frame['Text'] = data_frame['Text'].apply(process_text)\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse bloco de código realiza a divisão dos dados em conjuntos de treinamento e teste, aplica a subamostragem na classe majoritária do conjunto de teste, adiciona os exemplos extras da classe majoritária ao conjunto de treinamento e exibe informações sobre o tamanho dos conjuntos resultantes. Isso é feito para obter conjuntos de treinamento e teste balanceados em termos de classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : (197477,)\n",
      "Testing Data :  (14240,)\n"
     ]
    }
   ],
   "source": [
    "# Separe os campos de entrada (X) e o campo de saída (y)\n",
    "X = data_frame['Text']\n",
    "y = data_frame['oh_label']\n",
    "\n",
    "# Divisão dos dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=123)\n",
    "\n",
    "# Combine-os novamente para realizar a amostragem\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Separe as classes minoritárias e majoritárias\n",
    "minority_test = test_data[test_data.oh_label==1]\n",
    "majority_test = test_data[test_data.oh_label==0]\n",
    "\n",
    "# Subamostragem na classe majoritária do conjunto de teste\n",
    "majority_downsampled_test = resample(majority_test, \n",
    "                                     replace=False, \n",
    "                                     n_samples=len(minority_test), \n",
    "                                     random_state=123)\n",
    "\n",
    "# As amostras que não foram incluídas na subamostragem serão adicionadas ao conjunto de treinamento\n",
    "majority_train_extra = majority_test.loc[~majority_test.index.isin(majority_downsampled_test.index)]\n",
    "\n",
    "# Combine a classe minoritária com a classe majoritária subamostrada\n",
    "balanced_test = pd.concat([minority_test, majority_downsampled_test])\n",
    "\n",
    "# Combine o conjunto de treinamento existente com os exemplos extra da classe majoritária\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "balanced_train = pd.concat([train_data, majority_train_extra])\n",
    "\n",
    "# Separe X_test, y_test, X_train e y_train\n",
    "X_test = balanced_test['Text']\n",
    "y_test = balanced_test['oh_label']\n",
    "X_train = balanced_train['Text']\n",
    "y_train = balanced_train['oh_label']\n",
    "\n",
    "print('Training Data :', X_train.shape)\n",
    "print('Testing Data : ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas listas podem ser utilizadas posteriormente para armazenar informações ou resultados relacionados às amostras processadas ou classificadas. O objetivo específico do uso dessas listas não está claro neste trecho de código isolado. No entanto, elas podem ser utilizadas para armazenar informações como rótulos verdadeiros, previsões de um modelo de machine learning ou qualquer outra informação relevante durante o processamento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList = []\n",
    "resultList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No final dessas etapas, X_train_vec e X_test_vec conterão as representações TF-IDF dos dados de treinamento e teste, respectivamente. Essas representações numéricas podem ser usadas como entrada para modelos de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie o vetorizador TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajuste o vetorizador e transforme os dados de treinamento\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transforme os dados de teste\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trecho de código, um classificador Naive Bayes é criado e treinado para realizar a classificação de textos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do algoritmo Naive Bayes:  0.5440308988764045\n"
     ]
    }
   ],
   "source": [
    "# Crie o classificador Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Ajuste o modelo\n",
    "nb.fit(X_train_vec, y_train)\n",
    "\n",
    "# Calcule a acurácia no conjunto de teste\n",
    "print(\"Acurácia do algoritmo Naive Bayes: \", nb.score(X_test_vec, y_test))\n",
    "\n",
    "# adicionando resultado e label nas listas\n",
    "labelList.append(\"Naive_Byes\")\n",
    "resultList.append(nb.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trecho de código, é criado um classificador SVM (Support Vector Machine) para realizar a classificação de textos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print accuracy of svm algo:  0.7481741573033708\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(random_state=3)\n",
    "svm.fit(X_train_vec, y_train)\n",
    "print(\"print accuracy of svm algo: \",svm.score(X_test_vec, y_test))\n",
    "\n",
    "# adding result and label to lists\n",
    "labelList.append(\"SVM\")\n",
    "resultList.append(svm.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trecho de código, é criado um classificador Random Forest para realizar a classificação de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do algoritmo Random Forest:  0.723806179775281\n"
     ]
    }
   ],
   "source": [
    "# Crie o classificador Random Forest\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Ajuste o modelo\n",
    "rf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Calcule a acurácia no conjunto de teste\n",
    "print(\"Acurácia do algoritmo Random Forest: \", rf.score(X_test_vec, y_test))\n",
    "\n",
    "# adicionando resultado e label nas listas\n",
    "labelList.append(\"Random_Forest\")\n",
    "resultList.append(rf.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse trecho de código prepara e treina um modelo de Redes Neurais Recorrentes (RNN) para classificação de textos. O modelo é treinado utilizando os dados de treinamento e avaliado nos dados de teste. A acurácia do treinamento e do teste são impressas, e a acurácia do teste é adicionada à lista resultList."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 206803 unique tokens.\n",
      "Shape of data tensor: (159686, 250)\n",
      "Shape of label tensor: (159686, 2)\n",
      "(143717, 250) (143717, 2)\n",
      "(15969, 250) (15969, 2)\n",
      "Epoch 1/5\n",
      "2022/2022 [==============================] - 388s 191ms/step - loss: 0.1396 - accuracy: 0.9529 - val_loss: 0.1029 - val_accuracy: 0.9634\n",
      "Epoch 2/5\n",
      "2022/2022 [==============================] - 385s 190ms/step - loss: 0.0822 - accuracy: 0.9694 - val_loss: 0.1028 - val_accuracy: 0.9624\n",
      "Epoch 3/5\n",
      "2022/2022 [==============================] - 386s 191ms/step - loss: 0.0606 - accuracy: 0.9775 - val_loss: 0.1129 - val_accuracy: 0.9604\n",
      "Epoch 4/5\n",
      "2022/2022 [==============================] - 387s 192ms/step - loss: 0.0421 - accuracy: 0.9848 - val_loss: 0.1384 - val_accuracy: 0.9564\n",
      "Epoch 5/5\n",
      "2022/2022 [==============================] - 389s 192ms/step - loss: 0.0295 - accuracy: 0.9897 - val_loss: 0.1636 - val_accuracy: 0.9537\n",
      "Train Accuracy: 0.991\n",
      "Test Accuracy: 0.955\n"
     ]
    }
   ],
   "source": [
    "# Preparando a entrada para o modelo RNN\n",
    "MAX_NB_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(df['Text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['Text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "Y = pd.get_dummies(df['oh_label']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "# Definindo o modelo RNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Configurando o uso da GPU\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "# Treinando o modelo RNN\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Avaliando o modelo\n",
    "_, train_acc = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train Accuracy: %.3f' % train_acc)\n",
    "\n",
    "_, test_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f' % test_acc)\n",
    "\n",
    "# Adicionando resultado e label nas listas\n",
    "labelList.append(\"RNN\")\n",
    "resultList.append(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06c4d774ddc7539415fbff7166d4f51b885dc92dc708d40008f0c632c8b1a5f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
