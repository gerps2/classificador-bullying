{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Embedding, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse bloco de código lê os arquivos de dados, realiza o pré-processamento dos dados, equilibra as classes e limita a quantidade total de itens, preparando-os para uso em modelos de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index                                               Text  ed_label_0  \\\n",
      "87763  87763       `:You're no fun LSD.  No fun at all.       `    0.655172   \n",
      "25359  25359  `  == Copyright problems with  Image:Fernandez...    1.000000   \n",
      "42987  42987    == image ==  i have uploaded a better image,...    1.000000   \n",
      "51702  51702   :I'm afraid that doesn't pass WP:RS. The abov...    1.000000   \n",
      "79233  79233  I'm going by your comments on the content and ...    1.000000   \n",
      "\n",
      "       ed_label_1  oh_label  type_text            type_text_label   id  \\\n",
      "87763    0.344828       0.0        1.0                 aggression  NaN   \n",
      "25359    0.000000       0.0        1.0                 aggression  NaN   \n",
      "42987    0.000000       0.0        NaN  aggression_parsed_dataset  NaN   \n",
      "51702    0.000000       0.0        2.0                     attack  NaN   \n",
      "79233    0.000000       0.0        NaN  aggression_parsed_dataset  NaN   \n",
      "\n",
      "      Annotation  \n",
      "87763        NaN  \n",
      "25359        NaN  \n",
      "42987        NaN  \n",
      "51702        NaN  \n",
      "79233        NaN  \n"
     ]
    }
   ],
   "source": [
    "pasta_dados = 'dados'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "mapeamento_arquivos = {\n",
    "    'aggression.csv': 1,\n",
    "    'attack.csv': 2,\n",
    "    'racism.csv': 3,\n",
    "    'sexism.csv': 4,\n",
    "    'toxicity.csv': 5\n",
    "}\n",
    "\n",
    "\n",
    "for arquivo in os.listdir(pasta_dados):\n",
    "    if arquivo.endswith('.csv'):\n",
    "        caminho_arquivo = os.path.join(pasta_dados, arquivo)\n",
    "        df = pd.read_csv(caminho_arquivo)\n",
    "        \n",
    "        if arquivo in mapeamento_arquivos:\n",
    "            tipo_texto = mapeamento_arquivos[arquivo]\n",
    "            df['type_text'] = tipo_texto\n",
    "        \n",
    "        # Converter apenas as colunas numéricas para inteiros\n",
    "        colunas_numericas = df.select_dtypes(include='number').columns\n",
    "        df[colunas_numericas] = df[colunas_numericas].apply(pd.to_numeric, downcast='integer', errors='coerce')\n",
    "        \n",
    "        df['type_text_label'] = arquivo.replace(\".csv\", \"\")\n",
    "        dataframes.append(df)\n",
    "\n",
    "df_final = pd.concat(dataframes)\n",
    "\n",
    "# Balancear as classes e limitar a quantidade total de itens\n",
    "\n",
    "# Separa as classes majoritária e minoritária\n",
    "df_majority = df_final[df_final.oh_label==0]\n",
    "df_minority = df_final[df_final.oh_label==1]\n",
    "\n",
    "# Calcula a quantidade de itens para igualar as classes\n",
    "n_samples = 1000 // 2\n",
    "\n",
    "# Faz a subamostragem na classe majoritária\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=n_samples,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Faz a superamostragem na classe minoritária\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=n_samples,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Combina a classe minoritária com a classe majoritária\n",
    "df_final = pd.concat([df_majority_downsampled, df_minority_upsampled])\n",
    "\n",
    "print(df_final.head())\n",
    "# print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse trecho de código seleciona as colunas desejadas do dataframe \"df_final\" e cria um novo dataframe chamado \"data_frame\" contendo apenas essas colunas. Em seguida, ele imprime as primeiras linhas do novo dataframe para visualização. Isso pode ser útil para verificar se as colunas desejadas foram selecionadas corretamente e ter uma amostra dos dados que serão utilizados nas etapas seguintes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Text  oh_label  type_text  \\\n",
      "87763       `:You're no fun LSD.  No fun at all.       `       0.0        1.0   \n",
      "25359  `  == Copyright problems with  Image:Fernandez...       0.0        1.0   \n",
      "42987    == image ==  i have uploaded a better image,...       0.0        NaN   \n",
      "51702   :I'm afraid that doesn't pass WP:RS. The abov...       0.0        2.0   \n",
      "79233  I'm going by your comments on the content and ...       0.0        NaN   \n",
      "\n",
      "                 type_text_label  \n",
      "87763                 aggression  \n",
      "25359                 aggression  \n",
      "42987  aggression_parsed_dataset  \n",
      "51702                     attack  \n",
      "79233  aggression_parsed_dataset  \n"
     ]
    }
   ],
   "source": [
    "colunas_desejadas = ['Text', 'oh_label', 'type_text', 'type_text_label']\n",
    "data_frame = df_final[colunas_desejadas]\n",
    "print(data_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse bloco de código remove os valores nulos das colunas 'Text' e 'oh_label', em seguida verifica e remove os itens duplicados com base na coluna 'Text' no dataframe \"data_frame\". Isso é feito para garantir a consistência dos dados e evitar duplicações que possam interferir nos resultados da análise posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de itens duplicados antes da remoção: 3\n",
      "Quantidade de itens duplicados após a remoção: 0\n",
      "DataFrame sem itens duplicados:\n",
      "                                                    Text  oh_label  type_text  \\\n",
      "87763       `:You're no fun LSD.  No fun at all.       `       0.0        1.0   \n",
      "25359  `  == Copyright problems with  Image:Fernandez...       0.0        1.0   \n",
      "42987    == image ==  i have uploaded a better image,...       0.0        NaN   \n",
      "51702   :I'm afraid that doesn't pass WP:RS. The abov...       0.0        2.0   \n",
      "79233  I'm going by your comments on the content and ...       0.0        NaN   \n",
      "\n",
      "                 type_text_label  \n",
      "87763                 aggression  \n",
      "25359                 aggression  \n",
      "42987  aggression_parsed_dataset  \n",
      "51702                     attack  \n",
      "79233  aggression_parsed_dataset  \n"
     ]
    }
   ],
   "source": [
    "# Remova valores nulos das colunas 'Text' e 'type_text'\n",
    "data_frame = data_frame.dropna(subset=['Text', 'oh_label'])\n",
    "\n",
    "# Verifique a quantidade de itens duplicados antes da remoção\n",
    "qtd_duplicados = data_frame.duplicated(subset=['Text']).sum()\n",
    "print(\"Quantidade de itens duplicados antes da remoção:\", qtd_duplicados)\n",
    "\n",
    "# Remova itens duplicados pela coluna 'Text'\n",
    "data_frame = data_frame.drop_duplicates(subset=['Text'])\n",
    "\n",
    "# Verifique a quantidade de itens duplicados após a remoção\n",
    "qtd_duplicados = data_frame.duplicated(subset=['Text']).sum()\n",
    "print(\"Quantidade de itens duplicados após a remoção:\", qtd_duplicados)\n",
    "\n",
    "# Exiba o dataframe resultante\n",
    "print(\"DataFrame sem itens duplicados:\")\n",
    "print(data_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse bloco de código verifica e baixa os pacotes necessários do nltk, define uma função para processar o texto removendo caracteres não-alfabéticos, convertendo para minúsculas, tokenizando, removendo stopwords e lematizando as palavras. Em seguida, aplica essa função à coluna 'Text' do dataframe \"data_frame\" para processar os textos e exibe as primeiras linhas do dataframe resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gerso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gerso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gerso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>type_text</th>\n",
       "      <th>type_text_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87763</th>\n",
       "      <td>fun lsd fun</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25359</th>\n",
       "      <td>copyright problem image fernandez leandro jpg ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42987</th>\n",
       "      <td>image uploaded better image buti know put herp...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aggression_parsed_dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51702</th>\n",
       "      <td>afraid pas wp r two</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79233</th>\n",
       "      <td>going comment content source used fact pov sub...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aggression_parsed_dataset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  oh_label  type_text  \\\n",
       "87763                                        fun lsd fun       0.0        1.0   \n",
       "25359  copyright problem image fernandez leandro jpg ...       0.0        1.0   \n",
       "42987  image uploaded better image buti know put herp...       0.0        NaN   \n",
       "51702                                afraid pas wp r two       0.0        2.0   \n",
       "79233  going comment content source used fact pov sub...       0.0        NaN   \n",
       "\n",
       "                 type_text_label  \n",
       "87763                 aggression  \n",
       "25359                 aggression  \n",
       "42987  aggression_parsed_dataset  \n",
       "51702                     attack  \n",
       "79233  aggression_parsed_dataset  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifica se os pacotes do nltk já foram baixados\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))  # Coloca as stopwords num conjunto para buscar mais rápido\n",
    "\n",
    "def process_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)  # remove caracteres não-alfabéticos\n",
    "    text = text.lower()  # transforma o texto para minúsculo\n",
    "    words = nltk.word_tokenize(text)  # tokeniza o texto em palavras\n",
    "    words = [word for word in words if word not in stop_words]  # remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # lematiza as palavras\n",
    "    return ' '.join(words)\n",
    "\n",
    "data_frame['Text'] = data_frame['Text'].apply(process_text)\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse bloco de código realiza a divisão dos dados em conjuntos de treinamento e teste, aplica a subamostragem na classe majoritária do conjunto de teste, adiciona os exemplos extras da classe majoritária ao conjunto de treinamento e exibe informações sobre o tamanho dos conjuntos resultantes. Isso é feito para obter conjuntos de treinamento e teste balanceados em termos de classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : (667,)\n",
      "Testing Data :  (330,)\n"
     ]
    }
   ],
   "source": [
    "# Separe os campos de entrada (X) e o campo de saída (y)\n",
    "X = data_frame['Text']\n",
    "y = data_frame['oh_label']\n",
    "\n",
    "# Divisão dos dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=123)\n",
    "\n",
    "# Combine-os novamente para realizar a amostragem\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Separe as classes minoritárias e majoritárias\n",
    "minority_test = test_data[test_data.oh_label==1]\n",
    "majority_test = test_data[test_data.oh_label==0]\n",
    "\n",
    "# Subamostragem na classe majoritária do conjunto de teste\n",
    "majority_downsampled_test = resample(majority_test, \n",
    "                                     replace=False, \n",
    "                                     n_samples=len(minority_test), \n",
    "                                     random_state=123)\n",
    "\n",
    "# As amostras que não foram incluídas na subamostragem serão adicionadas ao conjunto de treinamento\n",
    "majority_train_extra = majority_test.loc[~majority_test.index.isin(majority_downsampled_test.index)]\n",
    "\n",
    "# Combine a classe minoritária com a classe majoritária subamostrada\n",
    "balanced_test = pd.concat([minority_test, majority_downsampled_test])\n",
    "\n",
    "# Combine o conjunto de treinamento existente com os exemplos extra da classe majoritária\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "balanced_train = pd.concat([train_data, majority_train_extra])\n",
    "\n",
    "# Separe X_test, y_test, X_train e y_train\n",
    "X_test = balanced_test['Text']\n",
    "y_test = balanced_test['oh_label']\n",
    "X_train = balanced_train['Text']\n",
    "y_train = balanced_train['oh_label']\n",
    "\n",
    "print('Training Data :', X_train.shape)\n",
    "print('Testing Data : ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas listas podem ser utilizadas posteriormente para armazenar informações ou resultados relacionados às amostras processadas ou classificadas. O objetivo específico do uso dessas listas não está claro neste trecho de código isolado. No entanto, elas podem ser utilizadas para armazenar informações como rótulos verdadeiros, previsões de um modelo de machine learning ou qualquer outra informação relevante durante o processamento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList = []\n",
    "resultList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No final dessas etapas, X_train_vec e X_test_vec conterão as representações TF-IDF dos dados de treinamento e teste, respectivamente. Essas representações numéricas podem ser usadas como entrada para modelos de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie o vetorizador TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajuste o vetorizador e transforme os dados de treinamento\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transforme os dados de teste\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trecho de código, um classificador Naive Bayes é criado e treinado para realizar a classificação de textos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do algoritmo Naive Bayes:  0.796969696969697\n"
     ]
    }
   ],
   "source": [
    "# Crie o classificador Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Ajuste o modelo\n",
    "nb.fit(X_train_vec, y_train)\n",
    "\n",
    "# Calcule a acurácia no conjunto de teste\n",
    "print(\"Acurácia do algoritmo Naive Bayes: \", nb.score(X_test_vec, y_test))\n",
    "\n",
    "# adicionando resultado e label nas listas\n",
    "labelList.append(\"Naive_Byes\")\n",
    "resultList.append(nb.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trecho de código, é criado um classificador SVM (Support Vector Machine) para realizar a classificação de textos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print accuracy of svm algo:  0.7787878787878788\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(random_state=3)\n",
    "svm.fit(X_train_vec, y_train)\n",
    "print(\"print accuracy of svm algo: \",svm.score(X_test_vec, y_test))\n",
    "\n",
    "# adding result and label to lists\n",
    "labelList.append(\"SVM\")\n",
    "resultList.append(svm.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trecho de código, é criado um classificador Random Forest para realizar a classificação de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do algoritmo Random Forest:  0.7818181818181819\n"
     ]
    }
   ],
   "source": [
    "# Crie o classificador Random Forest\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Ajuste o modelo\n",
    "rf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Calcule a acurácia no conjunto de teste\n",
    "print(\"Acurácia do algoritmo Random Forest: \", rf.score(X_test_vec, y_test))\n",
    "\n",
    "# adicionando resultado e label nas listas\n",
    "labelList.append(\"Random_Forest\")\n",
    "resultList.append(rf.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esse trecho de código prepara e treina um modelo de Redes Neurais Recorrentes (RNN) para classificação de textos. O modelo é treinado utilizando os dados de treinamento e avaliado nos dados de teste. A acurácia do treinamento e do teste são impressas, e a acurácia do teste é adicionada à lista resultList."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 206803 unique tokens.\n",
      "Shape of data tensor: (159686, 250)\n",
      "Shape of label tensor: (159686, 2)\n",
      "(143717, 250) (143717, 2)\n",
      "(15969, 250) (15969, 2)\n",
      "Epoch 1/5\n",
      " 500/2022 [======>.......................] - ETA: 5:12 - loss: 0.2123 - accuracy: 0.9327"
     ]
    }
   ],
   "source": [
    "# Preparando a entrada para o modelo RNN\n",
    "MAX_NB_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(df['Text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['Text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "Y = pd.get_dummies(df['oh_label']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "# Definindo o modelo RNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Configurando o uso da GPU\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "# Treinando o modelo RNN\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Avaliando o modelo\n",
    "_, train_acc = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train Accuracy: %.3f' % train_acc)\n",
    "\n",
    "_, test_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f' % test_acc)\n",
    "\n",
    "# Adicionando resultado e label nas listas\n",
    "labelList.append(\"RNN\")\n",
    "resultList.append(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06c4d774ddc7539415fbff7166d4f51b885dc92dc708d40008f0c632c8b1a5f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
