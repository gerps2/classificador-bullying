{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       index  \\\n",
      "81847                  81847   \n",
      "66583                  66583   \n",
      "42466                  42466   \n",
      "71897                  71897   \n",
      "209    5.76464759282741E+017   \n",
      "\n",
      "                                                    Text  ed_label_0  \\\n",
      "81847  ` **I am not sure which popularity you are ref...    1.000000   \n",
      "66583    == diablada = bolivia ==  look at maps. this...    0.777778   \n",
      "42466    == Re: You ==  Your blocking me for no reaso...    0.700000   \n",
      "71897  big risks, yes, he bribed many persons, but he...    1.000000   \n",
      "209       @cperciva @femfreq ah. well. no one's perfect.         NaN   \n",
      "\n",
      "       ed_label_1  oh_label  type_text type_text_label                     id  \\\n",
      "81847    0.000000       0.0          2          attack                    NaN   \n",
      "66583    0.222222       0.0          1      aggression                    NaN   \n",
      "42466    0.300000       0.0          5        toxicity                    NaN   \n",
      "71897    0.000000       0.0          2          attack                    NaN   \n",
      "209           NaN       0.0          4          sexism  5.76464759282741E+017   \n",
      "\n",
      "      Annotation  \n",
      "81847        NaN  \n",
      "66583        NaN  \n",
      "42466        NaN  \n",
      "71897        NaN  \n",
      "209         none  \n"
     ]
    }
   ],
   "source": [
    "pasta_dados = 'dados'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "mapeamento_arquivos = {\n",
    "    'aggression.csv': 1,\n",
    "    'attack.csv': 2,\n",
    "    'racism.csv': 3,\n",
    "    'sexism.csv': 4,\n",
    "    'toxicity.csv': 5\n",
    "}\n",
    "\n",
    "\n",
    "for arquivo in os.listdir(pasta_dados):\n",
    "    if arquivo.endswith('.csv'):\n",
    "        caminho_arquivo = os.path.join(pasta_dados, arquivo)\n",
    "        df = pd.read_csv(caminho_arquivo)\n",
    "        \n",
    "        if arquivo in mapeamento_arquivos:\n",
    "            tipo_texto = mapeamento_arquivos[arquivo]\n",
    "            df['type_text'] = tipo_texto\n",
    "        \n",
    "        # Converter apenas as colunas numéricas para inteiros\n",
    "        colunas_numericas = df.select_dtypes(include='number').columns\n",
    "        df[colunas_numericas] = df[colunas_numericas].apply(pd.to_numeric, downcast='integer', errors='coerce')\n",
    "        \n",
    "        df['type_text_label'] = arquivo.replace(\".csv\", \"\")\n",
    "        dataframes.append(df)\n",
    "\n",
    "df_final = pd.concat(dataframes)\n",
    "\n",
    "# Balancear as classes e limitar a quantidade total de itens\n",
    "\n",
    "# Separa as classes majoritária e minoritária\n",
    "df_majority = df_final[df_final.oh_label==0]\n",
    "df_minority = df_final[df_final.oh_label==1]\n",
    "\n",
    "# Calcula a quantidade de itens para igualar as classes\n",
    "n_samples = 1000 // 2\n",
    "\n",
    "# Faz a subamostragem na classe majoritária\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=n_samples,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Faz a superamostragem na classe minoritária\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=n_samples,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Combina a classe minoritária com a classe majoritária\n",
    "df_final = pd.concat([df_majority_downsampled, df_minority_upsampled])\n",
    "\n",
    "print(df_final.head())\n",
    "# print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Text  oh_label  type_text  \\\n",
      "81847  ` **I am not sure which popularity you are ref...       0.0          2   \n",
      "66583    == diablada = bolivia ==  look at maps. this...       0.0          1   \n",
      "42466    == Re: You ==  Your blocking me for no reaso...       0.0          5   \n",
      "71897  big risks, yes, he bribed many persons, but he...       0.0          2   \n",
      "209       @cperciva @femfreq ah. well. no one's perfect.       0.0          4   \n",
      "\n",
      "      type_text_label  \n",
      "81847          attack  \n",
      "66583      aggression  \n",
      "42466        toxicity  \n",
      "71897          attack  \n",
      "209            sexism  \n"
     ]
    }
   ],
   "source": [
    "colunas_desejadas = ['Text', 'oh_label', 'type_text', 'type_text_label']\n",
    "data_frame = df_final[colunas_desejadas]\n",
    "print(data_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de itens duplicados antes da remoção: 7\n",
      "Quantidade de itens duplicados após a remoção: 0\n",
      "DataFrame sem itens duplicados:\n",
      "                                                    Text  oh_label  type_text  \\\n",
      "81847  ` **I am not sure which popularity you are ref...       0.0          2   \n",
      "66583    == diablada = bolivia ==  look at maps. this...       0.0          1   \n",
      "42466    == Re: You ==  Your blocking me for no reaso...       0.0          5   \n",
      "71897  big risks, yes, he bribed many persons, but he...       0.0          2   \n",
      "209       @cperciva @femfreq ah. well. no one's perfect.       0.0          4   \n",
      "\n",
      "      type_text_label  \n",
      "81847          attack  \n",
      "66583      aggression  \n",
      "42466        toxicity  \n",
      "71897          attack  \n",
      "209            sexism  \n"
     ]
    }
   ],
   "source": [
    "# Remova valores nulos das colunas 'Text' e 'type_text'\n",
    "data_frame = data_frame.dropna(subset=['Text', 'oh_label'])\n",
    "\n",
    "# Verifique a quantidade de itens duplicados antes da remoção\n",
    "qtd_duplicados = data_frame.duplicated(subset=['Text']).sum()\n",
    "print(\"Quantidade de itens duplicados antes da remoção:\", qtd_duplicados)\n",
    "\n",
    "# Remova itens duplicados pela coluna 'Text'\n",
    "data_frame = data_frame.drop_duplicates(subset=['Text'])\n",
    "\n",
    "# Verifique a quantidade de itens duplicados após a remoção\n",
    "qtd_duplicados = data_frame.duplicated(subset=['Text']).sum()\n",
    "print(\"Quantidade de itens duplicados após a remoção:\", qtd_duplicados)\n",
    "\n",
    "# Exiba o dataframe resultante\n",
    "print(\"DataFrame sem itens duplicados:\")\n",
    "print(data_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gerson.prudencio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gerson.prudencio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gerson.prudencio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>type_text</th>\n",
       "      <th>type_text_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81847</th>\n",
       "      <td>sure popularity referring please clarify recen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66583</th>\n",
       "      <td>diablada bolivia look map show truth oruro sta...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42466</th>\n",
       "      <td>blocking reason fell unfail abuse continue fig...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>toxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71897</th>\n",
       "      <td>big risk yes bribed many person also many good...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>cperciva femfreq ah well one perfect</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  oh_label  type_text  \\\n",
       "81847  sure popularity referring please clarify recen...       0.0          2   \n",
       "66583  diablada bolivia look map show truth oruro sta...       0.0          1   \n",
       "42466  blocking reason fell unfail abuse continue fig...       0.0          5   \n",
       "71897  big risk yes bribed many person also many good...       0.0          2   \n",
       "209                 cperciva femfreq ah well one perfect       0.0          4   \n",
       "\n",
       "      type_text_label  \n",
       "81847          attack  \n",
       "66583      aggression  \n",
       "42466        toxicity  \n",
       "71897          attack  \n",
       "209            sexism  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifica se os pacotes do nltk já foram baixados\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))  # Coloca as stopwords num conjunto para buscar mais rápido\n",
    "\n",
    "def process_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)  # remove caracteres não-alfabéticos\n",
    "    text = text.lower()  # transforma o texto para minúsculo\n",
    "    words = nltk.word_tokenize(text)  # tokeniza o texto em palavras\n",
    "    words = [word for word in words if word not in stop_words]  # remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # lematiza as palavras\n",
    "    return ' '.join(words)\n",
    "\n",
    "data_frame['Text'] = data_frame['Text'].apply(process_text)\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : (667,)\n",
      "Testing Data :  (326,)\n"
     ]
    }
   ],
   "source": [
    "# Separe os campos de entrada (X) e o campo de saída (y)\n",
    "X = data_frame['Text']\n",
    "y = data_frame['oh_label']\n",
    "\n",
    "# Divisão dos dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=123)\n",
    "\n",
    "# Combine-os novamente para realizar a amostragem\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Separe as classes minoritárias e majoritárias\n",
    "minority_test = test_data[test_data.oh_label==1]\n",
    "majority_test = test_data[test_data.oh_label==0]\n",
    "\n",
    "# Subamostragem na classe majoritária do conjunto de teste\n",
    "majority_downsampled_test = resample(majority_test, \n",
    "                                     replace=False, \n",
    "                                     n_samples=len(minority_test), \n",
    "                                     random_state=123)\n",
    "\n",
    "# As amostras que não foram incluídas na subamostragem serão adicionadas ao conjunto de treinamento\n",
    "majority_train_extra = majority_test.loc[~majority_test.index.isin(majority_downsampled_test.index)]\n",
    "\n",
    "# Combine a classe minoritária com a classe majoritária subamostrada\n",
    "balanced_test = pd.concat([minority_test, majority_downsampled_test])\n",
    "\n",
    "# Combine o conjunto de treinamento existente com os exemplos extra da classe majoritária\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "balanced_train = pd.concat([train_data, majority_train_extra])\n",
    "\n",
    "# Separe X_test, y_test, X_train e y_train\n",
    "X_test = balanced_test['Text']\n",
    "y_test = balanced_test['oh_label']\n",
    "X_train = balanced_train['Text']\n",
    "y_train = balanced_train['oh_label']\n",
    "\n",
    "print('Training Data :', X_train.shape)\n",
    "print('Testing Data : ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList = []\n",
    "resultList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie o vetorizador TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajuste o vetorizador e transforme os dados de treinamento\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transforme os dados de teste\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do algoritmo Naive Bayes:  0.803680981595092\n"
     ]
    }
   ],
   "source": [
    "# Crie o classificador Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Ajuste o modelo\n",
    "nb.fit(X_train_vec, y_train)\n",
    "\n",
    "# Calcule a acurácia no conjunto de teste\n",
    "print(\"Acurácia do algoritmo Naive Bayes: \", nb.score(X_test_vec, y_test))\n",
    "\n",
    "# adicionando resultado e label nas listas\n",
    "labelList.append(\"Naive_Byes\")\n",
    "resultList.append(nb.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print accuracy of svm algo:  0.7760736196319018\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(random_state=3)\n",
    "svm.fit(X_train_vec, y_train)\n",
    "print(\"print accuracy of svm algo: \",svm.score(X_test_vec, y_test))\n",
    "\n",
    "# adding result and label to lists\n",
    "labelList.append(\"SVM\")\n",
    "resultList.append(svm.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do algoritmo Random Forest:  0.7116564417177914\n"
     ]
    }
   ],
   "source": [
    "# Crie o classificador Random Forest\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Ajuste o modelo\n",
    "rf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Calcule a acurácia no conjunto de teste\n",
    "print(\"Acurácia do algoritmo Random Forest: \", rf.score(X_test_vec, y_test))\n",
    "\n",
    "# adicionando resultado e label nas listas\n",
    "labelList.append(\"Random_Forest\")\n",
    "resultList.append(rf.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 206803 unique tokens.\n",
      "Shape of data tensor: (159686, 250)\n",
      "Shape of label tensor: (159686, 2)\n",
      "(143717, 250) (143717, 2)\n",
      "(15969, 250) (15969, 2)\n",
      "Epoch 1/5\n",
      " 941/2022 [============>.................] - ETA: 6:49 - loss: 0.1690 - accuracy: 0.9455"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Treinando o modelo RNN\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Avaliando o modelo\u001b[39;00m\n\u001b[0;32m     50\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_train, Y_train, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gerson.prudencio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\gerson.prudencio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\gerson.prudencio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\gerson.prudencio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\gerson.prudencio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\gerson.prudencio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\gerson.prudencio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\gerson.prudencio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\gerson.prudencio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preparando a entrada para o modelo RNN\n",
    "# O número máximo de palavras a serem usadas. (mais frequentes)\n",
    "MAX_NB_WORDS = 50000\n",
    "\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(df['Text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['Text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "Y = pd.get_dummies(df['oh_label']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "# Definindo o modelo RNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Tentar usar diferentes otimizadores e diferentes configurações de otimizador\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo RNN\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Avaliando o modelo\n",
    "train_loss, train_acc = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train Accuracy: %.3f' % train_acc)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f' % test_acc)\n",
    "\n",
    "# Adicionando resultado e label nas listas\n",
    "labelList.append(\"RNN\")\n",
    "resultList.append(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e2c49d0c028a5ee69bf2654349fbb394914b60ca4407d83485d6f82b3b71b61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
