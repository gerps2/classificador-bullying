{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Text</th>\n",
       "      <th>ed_label_0</th>\n",
       "      <th>ed_label_1</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>type_text</th>\n",
       "      <th>type_text_label</th>\n",
       "      <th>id</th>\n",
       "      <th>Annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>`- This is not ``creative``.  Those are the di...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>aggression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>`  :: the term ``standard model`` is itself le...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>aggression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>aggression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>aggression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>aggression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                                               Text  ed_label_0  \\\n",
       "0     0  `- This is not ``creative``.  Those are the di...    0.900000   \n",
       "1     1  `  :: the term ``standard model`` is itself le...    1.000000   \n",
       "2     2    True or false, the situation as of March 200...    1.000000   \n",
       "3     3   Next, maybe you could work on being less cond...    0.555556   \n",
       "4     4               This page will need disambiguation.     1.000000   \n",
       "\n",
       "   ed_label_1  oh_label  type_text type_text_label   id Annotation  \n",
       "0    0.100000       0.0          1      aggression  NaN        NaN  \n",
       "1    0.000000       0.0          1      aggression  NaN        NaN  \n",
       "2    0.000000       0.0          1      aggression  NaN        NaN  \n",
       "3    0.444444       0.0          1      aggression  NaN        NaN  \n",
       "4    0.000000       0.0          1      aggression  NaN        NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pasta_dados = 'dados'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "mapeamento_arquivos = {\n",
    "    'aggression.csv': 1,\n",
    "    'attack.csv': 2,\n",
    "    'racism.csv': 3,\n",
    "    'sexism.csv': 4,\n",
    "    'toxicity.csv': 5\n",
    "}\n",
    "\n",
    "\n",
    "for arquivo in os.listdir(pasta_dados):\n",
    "    if arquivo.endswith('.csv'):\n",
    "        caminho_arquivo = os.path.join(pasta_dados, arquivo)\n",
    "        df = pd.read_csv(caminho_arquivo)\n",
    "        \n",
    "        if arquivo in mapeamento_arquivos:\n",
    "            tipo_texto = mapeamento_arquivos[arquivo]\n",
    "            df['type_text'] = tipo_texto\n",
    "        \n",
    "        colunas_numericas = df.select_dtypes(include='number').columns\n",
    "        df[colunas_numericas] = df[colunas_numericas].apply(pd.to_numeric, downcast='integer', errors='coerce')\n",
    "        \n",
    "        df['type_text_label'] = arquivo.replace(\".csv\", \"\")\n",
    "        dataframes.append(df)\n",
    "\n",
    "df_final = pd.concat(dataframes)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remover textos nulos ou sem valor\n",
    "df_filtered = df_final.dropna(subset=['Text', 'oh_label'])\n",
    "\n",
    "# Agrupar e contar a quantidade de itens por classe em cada DataFrame filtrado\n",
    "grouped_counts = df_filtered.groupby(['oh_label', 'type_text_label']).size().reset_index(name='count')\n",
    "\n",
    "# Filtrar apenas os grupos onde oh_label é igual a 1\n",
    "filtro = grouped_counts['oh_label'] == 1\n",
    "grouped_counts = grouped_counts.loc[filtro]\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "# Iterar sobre cada grupo\n",
    "for group, count in grouped_counts.groupby(['oh_label', 'type_text_label']):\n",
    "    # Obter o oh_label e type_text_label do grupo atual\n",
    "    oh_label = group[0]\n",
    "    type_text_label = group[1]\n",
    "    \n",
    "    filter_yes_bullying = (0, type_text_label)\n",
    "    filter_not_bullying = (1, type_text_label)\n",
    "    \n",
    "    count_yes_bullying = count[count['type_text_label'] == type_text_label]['count'].values[0]\n",
    "    count_not_bullying = count[count['type_text_label'] == type_text_label]['count'].values[0]\n",
    "    \n",
    "    group_yes_bullying = df_filtered[(df_filtered['oh_label'] == filter_yes_bullying[0]) & (df_filtered['type_text_label'] == filter_yes_bullying[1])].sample(n=count_yes_bullying, random_state=42)\n",
    "    group_not_bullying = df_filtered[(df_filtered['oh_label'] == filter_not_bullying[0]) & (df_filtered['type_text_label'] == filter_not_bullying[1])].sample(n=count_not_bullying, random_state=42)\n",
    "    \n",
    "    df_new = pd.concat([df_new, group_yes_bullying])\n",
    "    df_new = pd.concat([df_new, group_not_bullying])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de dados carregado.\n",
      "Pré-processamento de texto concluído.\n",
      "adicionando coluna de pre processamento para o rnn, pois a Vetorização e diferente\n",
      "Vetorização TF-IDF concluída.\n",
      "Conjunto de dados dividido em treinamento, teste e validação.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Carregar o conjunto de dados\n",
    "df = df_new\n",
    "print(\"Conjunto de dados carregado.\")\n",
    "\n",
    "# Pré-processamento de texto\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "df['preprocessed_text'] = df['Text'].apply(preprocess_text)\n",
    "print(\"Pré-processamento de texto concluído.\")\n",
    "\n",
    "df['preprocessed_text_rnn'] = df['preprocessed_text']\n",
    "print(\"adicionando coluna de pre processamento para o rnn, pois a Vetorização e diferente\")\n",
    "\n",
    "# Vetorização TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X = vectorizer.fit_transform(df['preprocessed_text'])\n",
    "y = df['oh_label']\n",
    "print(\"Vetorização TF-IDF concluída.\")\n",
    "\n",
    "# Divisão do conjunto de dados em treinamento, teste e validação\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, stratify=y_train, random_state=42)\n",
    "print(\"Conjunto de dados dividido em treinamento, teste e validação.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento do modelo Naive Bayes concluído.\n"
     ]
    }
   ],
   "source": [
    "# Treinamento do modelo Naive Bayes\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "print(\"Treinamento do modelo Naive Bayes concluído.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1074/1074 [==============================] - 226s 209ms/step - loss: 0.3239 - accuracy: 0.8624 - val_loss: 0.2463 - val_accuracy: 0.9002\n",
      "Epoch 2/5\n",
      "1074/1074 [==============================] - 219s 204ms/step - loss: 0.1927 - accuracy: 0.9261 - val_loss: 0.2471 - val_accuracy: 0.9027\n",
      "Epoch 3/5\n",
      "1074/1074 [==============================] - 212s 197ms/step - loss: 0.1374 - accuracy: 0.9483 - val_loss: 0.2826 - val_accuracy: 0.9005\n",
      "Epoch 4/5\n",
      "1074/1074 [==============================] - 212s 197ms/step - loss: 0.0998 - accuracy: 0.9603 - val_loss: 0.3052 - val_accuracy: 0.9003\n",
      "Epoch 5/5\n",
      "1074/1074 [==============================] - 211s 197ms/step - loss: 0.0759 - accuracy: 0.9690 - val_loss: 0.3561 - val_accuracy: 0.8976\n",
      "Treinamento do modelo RNN concluído.\n"
     ]
    }
   ],
   "source": [
    "# Divisão do conjunto de dados em treinamento, teste e validação para RNN\n",
    "X_rnn = df['preprocessed_text_rnn']\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X_rnn, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train_rnn, X_val_rnn, y_train_rnn, y_val_rnn = train_test_split(X_train_rnn, y_train_rnn, test_size=0.125, stratify=y_train_rnn, random_state=42)\n",
    "\n",
    "# Treinamento do Tokenizer e transformação dos dados em sequências\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_rnn)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train_rnn)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test_rnn)\n",
    "sequences_val = tokenizer.texts_to_sequences(X_val_rnn)\n",
    "\n",
    "max_len = 100  # comprimento máximo das sequências\n",
    "X_train_rnn = pad_sequences(sequences_train, maxlen=max_len)\n",
    "X_test_rnn = pad_sequences(sequences_test, maxlen=max_len)\n",
    "X_val_rnn = pad_sequences(sequences_val, maxlen=max_len)\n",
    "\n",
    "# Convertendo labels para numpy arrays\n",
    "y_train_rnn = np.array(y_train_rnn)\n",
    "y_test_rnn = np.array(y_test_rnn)\n",
    "y_val_rnn = np.array(y_val_rnn)\n",
    "\n",
    "# Treinamento do modelo RNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_len))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_rnn, y_train_rnn, validation_data=(X_val_rnn, y_val_rnn), epochs=5, batch_size=64)\n",
    "print(\"Treinamento do modelo RNN concluído.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo SVM com busca em grade\n",
    "param_grid = {'C': [0.1, 1, 10],'kernel': ['linear', 'rbf'],'gamma': ['scale', 'auto']}\n",
    "svm_classifier = SVC(probability=True)\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_svm_classifier = grid_search.best_estimator_\n",
    "print(\"Treinamento do modelo SVM concluído.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação dos modelos\n",
    "svm_predictions = best_svm_classifier.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "nb_predictions = nb_classifier.predict(X_test)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "rnn_predictions = model.predict(X_test)\n",
    "rnn_predictions_labels = np.round(rnn_predictions).flatten()\n",
    "rnn_accuracy = accuracy_score(y_test, rnn_predictions_labels)\n",
    "print(\"Avaliação dos modelos concluída.\")\n",
    "\n",
    "# Exibição dos resultados\n",
    "print('Acurácia SVM:', svm_accuracy)\n",
    "print('Acurácia Naive Bayes:', nb_accuracy)\n",
    "print('Acurácia RNN:', rnn_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06c4d774ddc7539415fbff7166d4f51b885dc92dc708d40008f0c632c8b1a5f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
